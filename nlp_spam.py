# -*- coding: utf-8 -*-
"""NLP-spam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rjipfr1kP_9B6VYexIOzj2dgZ3YBR-c8

# NLP-Spam or Not Spam

## Data Preprocessing
"""

import os
os.environ['KAGGLE_USERNAME'] = #'userid'
os.environ['KAGGLE_KEY'] = #'key'

!kaggle datasets download -d datatattle/email-classification-nlp

!unzip -q email-classification-nlp.zip -d .

import pandas as pd
import numpy as np

df = pd.read_csv('SMS_train.csv', encoding= 'unicode_escape')
df.info()

df.Label[df.Label == 'Spam'] = 1 
df.Label[df.Label == 'Non-Spam'] = 0
df.head()

df = df.astype({'Label':float})
df.dtypes

x = df['Message_body'].values
y = df['Label'].values

#split train test
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

#tokenization
from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(x_train) 
tokenizer.fit_on_texts(x_test)

#feature scaling
from tensorflow.keras.preprocessing.sequence import pad_sequences

sequence_train = tokenizer.texts_to_sequences(x_train)
sequence_test = tokenizer.texts_to_sequences(x_test)
 
padded_train = pad_sequences(sequence_train) 
padded_test = pad_sequences(sequence_test)

"""## Create Sequential Model using LSTM"""

import tensorflow as tf
#create model
nlp = tf.keras.Sequential([                       
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

#compile
nlp.compile(loss='binary_crossentropy',
            optimizer='adam',
            metrics=['accuracy'])

"""## Train the Model"""

#callback function
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.9 and logs.get('val_accuracy')>0.9):
      print("\nAccuracy has reached > 90%!")
      self.model.stop_training = True
callbacks = myCallback()

history = nlp.fit(padded_train,
                  y_train,
                  epochs=50, 
                  validation_data=(padded_test, y_test),
                  verbose=2,
                  callbacks=[callbacks])

"""## Plotting Loss and Accuracy"""

import matplotlib.pyplot as plt

#plot loss vs epochs
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')

plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend()

plt.show()

#plot accuracy vs epochs
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')

plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()

plt.show()

"""## Conclusion"""

#With simple datasets (<1000 sample and 2 label), using this model we can have accuracy > 90% with only 3 epochs